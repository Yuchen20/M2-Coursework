

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Understanding FLOPS Calculation for Qwen2.5 Models &mdash; Time Series Forecasting with LLMs beta documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
      <link rel="stylesheet" type="text/css" href="../_static/nbsphinx-code-cells.css?v=2aa19091" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a6e103b4" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=f0d2c090"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Evaluating Untrained LLM Performance on Time Series Forecasting" href="3_untrained_behaviour.html" />
    <link rel="prev" title="Lotka-Volterra Dataset Exploration &amp; LLMTIME Preprocessing" href="1_dataset_preprocess.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Time Series Forecasting with LLMs
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Jupyter Notebooks</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="1_dataset_preprocess.html">Lotka-Volterra Dataset Exploration &amp; LLMTIME Preprocessing</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Understanding FLOPS Calculation for Qwen2.5 Models</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Key-Components">Key Components</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Defining-Operation-Costs">Defining Operation Costs</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Model-Architecture-Parameters">Model Architecture Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#1.-Multi-head-Attention-FLOPS">1. Multi-head Attention FLOPS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#2.-RMSNorm-FLOPS">2. RMSNorm FLOPS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#3.-Feed-Forward-Network-FLOPS">3. Feed-Forward Network FLOPS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#4.-LoRA-Adaptation-FLOPS">4. LoRA Adaptation FLOPS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#5.-Language-Model-Head-FLOPS">5. Language Model Head FLOPS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#6.-Loss-Calculation-FLOPS">6. Loss Calculation FLOPS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#7.-Residual-Connection-FLOPS">7. Residual Connection FLOPS</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Full-Model-FLOPS-Breakdown">Full Model FLOPS Breakdown</a></li>
<li class="toctree-l2"><a class="reference internal" href="#FLOPS-Scaling-with-Different-Parameters">FLOPS Scaling with Different Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Budget-Planning-Analysis">Budget Planning Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="3_untrained_behaviour.html">Evaluating Untrained LLM Performance on Time Series Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_train_lora_llm.html">Train LoRA-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="5_initial_train_behaviour.html">Evaluating Trained LLM Performance on Time Series Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="6_fully_trained_behaviour.html">Evaluating Fully Trained LLM Performance on Time Series Forecasting</a></li>
<li class="toctree-l1"><a class="reference internal" href="6_fully_trained_behaviour.html#Flops-Analysis">Flops Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="7_weight_visualize.html">LM Bias Weight Visualization</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Time Series Forecasting with LLMs</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Understanding FLOPS Calculation for Qwen2.5 Models</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/notebooks/2_flops_calculation.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="admonition note">
<p class="admonition-title">Note</p>
<p>This page was generated from a Jupyter notebook.
<a class="reference external" href="https://github.com/yourusername/time_series_llm/blob/main/notebooks2_flops_calculation.ipynb">View the original notebook</a></p>
</div>
<section id="Understanding-FLOPS-Calculation-for-Qwen2.5-Models">
<h1>Understanding FLOPS Calculation for Qwen2.5 Models<a class="headerlink" href="#Understanding-FLOPS-Calculation-for-Qwen2.5-Models" title="Link to this heading"></a></h1>
<p>This notebook provides a detailed explanation of how we calculate Floating Point Operations (FLOPS) for the Qwen2.5 model architecture with LoRA fine-tuning. FLOPS counting is crucial for estimating computational requirements and adhering to the 10^17 FLOPS budget specified in the coursework.</p>
<section id="Key-Components">
<h2>Key Components<a class="headerlink" href="#Key-Components" title="Link to this heading"></a></h2>
<p>The Qwen2.5-0.5B model architecture consists of:</p>
<ul class="simple">
<li><p>24 transformer layers</p></li>
<li><p>896 hidden dimensions</p></li>
<li><p>14 attention heads</p></li>
<li><p>64 head dimensions</p></li>
<li><p>4864 feed-forward network dimensions</p></li>
<li><p>151936 vocabulary size</p></li>
<li><p>128 attention dimensions</p></li>
</ul>
<p>Our FLOPS calculator accounts for operations in:</p>
<ol class="arabic simple">
<li><p>Multi-head attention (MHA)</p></li>
<li><p>RMSNorm layer normalization</p></li>
<li><p>Feed-forward network (FFN)</p></li>
<li><p>Residual connections</p></li>
<li><p>LoRA adaptations (during training)</p></li>
<li><p>Language model head</p></li>
<li><p>Loss calculation</p></li>
</ol>
</section>
<section id="Defining-Operation-Costs">
<h2>Defining Operation Costs<a class="headerlink" href="#Defining-Operation-Costs" title="Link to this heading"></a></h2>
<p>Based on the coursework specification, we count FLOPS with the following operation costs:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">dataclasses</span><span class="w"> </span><span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="c1"># Add the src directory to the path to import the QwenFlopsCalculator</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">)),</span> <span class="s1">&#39;src&#39;</span><span class="p">))</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">get_flops</span><span class="w"> </span><span class="kn">import</span> <span class="n">QwenFlopsCalculator</span><span class="p">,</span> <span class="n">OperationFlops</span>

<span class="c1"># Create a flops calculator instance</span>
<span class="n">flops_calculator</span> <span class="o">=</span> <span class="n">QwenFlopsCalculator</span><span class="p">()</span>

<span class="c1"># Display the operation costs</span>
<span class="nd">@dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">OperationFlops</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    FLOPs for different operations as specified in the coursework:</span>
<span class="sd">        Additions, Subtractions, Negations: 1</span>
<span class="sd">        Multiplications, Divisions, Inverses: 1</span>
<span class="sd">        ReLU, Absolute Value: 1</span>
<span class="sd">        Exponentiation, Logarithm: 10</span>
<span class="sd">        Sine, Cosine, Square Root: 10</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_ADD</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">_SUB</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">_NEG</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">_MUL</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">_DIV</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">_INV</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">_RELU</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">_ABS</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">_EXP</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">_LOG</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">_SIN</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">_COS</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="n">_SQRT</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Operation FLOPS costs used in calculations:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">attr_name</span><span class="p">,</span> <span class="n">attr_value</span> <span class="ow">in</span> <span class="n">OperationFlops</span><span class="p">()</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">attr_name</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">attr_name</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="si">:</span><span class="s2">10</span><span class="si">}</span><span class="s2"> : </span><span class="si">{</span><span class="n">attr_value</span><span class="si">}</span><span class="s2"> FLOPS&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Operation FLOPS costs used in calculations:
ADD        : 1 FLOPS
SUB        : 1 FLOPS
NEG        : 1 FLOPS
MUL        : 1 FLOPS
DIV        : 1 FLOPS
INV        : 1 FLOPS
RELU       : 1 FLOPS
ABS        : 1 FLOPS
EXP        : 10 FLOPS
LOG        : 10 FLOPS
SIN        : 10 FLOPS
COS        : 10 FLOPS
SQRT       : 10 FLOPS
</pre></div></div>
</div>
</section>
<section id="Model-Architecture-Parameters">
<h2>Model Architecture Parameters<a class="headerlink" href="#Model-Architecture-Parameters" title="Link to this heading"></a></h2>
<p>The Qwen2.5-0.5B model has these specific architecture parameters:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model Architecture Parameters:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Number of layers:&#39;</span><span class="si">:</span><span class="s2">30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">flops_calculator</span><span class="o">.</span><span class="n">num_layers</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Hidden dimension:&#39;</span><span class="si">:</span><span class="s2">30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">flops_calculator</span><span class="o">.</span><span class="n">hidden_dim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Number of attention heads:&#39;</span><span class="si">:</span><span class="s2">30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">flops_calculator</span><span class="o">.</span><span class="n">num_heads</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Head dimension:&#39;</span><span class="si">:</span><span class="s2">30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">flops_calculator</span><span class="o">.</span><span class="n">head_dim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Feed-forward dimension:&#39;</span><span class="si">:</span><span class="s2">30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">flops_calculator</span><span class="o">.</span><span class="n">ffn_dim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Vocabulary size:&#39;</span><span class="si">:</span><span class="s2">30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">flops_calculator</span><span class="o">.</span><span class="n">vocab_size</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Attention dimension:&#39;</span><span class="si">:</span><span class="s2">30</span><span class="si">}</span><span class="s2"> </span><span class="si">{</span><span class="n">flops_calculator</span><span class="o">.</span><span class="n">attention_dim</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Model Architecture Parameters:
Number of layers:              24
Hidden dimension:              896
Number of attention heads:     14
Head dimension:                64
Feed-forward dimension:        4864
Vocabulary size:               151936
Attention dimension:           128
</pre></div></div>
</div>
</section>
<section id="1.-Multi-head-Attention-FLOPS">
<h2>1. Multi-head Attention FLOPS<a class="headerlink" href="#1.-Multi-head-Attention-FLOPS" title="Link to this heading"></a></h2>
<p>Now let’s examine how FLOPS are calculated for the multi-head attention mechanism. The calculation includes:</p>
<ol class="arabic simple">
<li><p>Query, Key, and Value projections</p></li>
<li><p>Rotary positional embedding application （simplified - only accounting for the flops of addition)</p></li>
<li><p>Attention scores (QK^T) computation</p></li>
<li><p>Scaling and softmax operations</p></li>
<li><p>Attention application to values</p></li>
<li><p>Output projection</p></li>
</ol>
<p>Let’s use a small example with batch_size=1 and seq_len=32:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">verbose</span> <span class="o">=</span> <span class="kc">True</span>

<span class="n">attention_flops</span><span class="p">,</span> <span class="n">attention_breakdown</span> <span class="o">=</span> <span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_attention_flops</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">verbose</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">attention_breakdown</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total Attention FLOPS: </span><span class="si">{</span><span class="n">attention_flops</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Attention FLOPs Breakdown:
    QKV Projection:        308,281,344 FLOPs - Matrix multiplication to project embeddings to QKV
    Rotary Embedding:      114,688 FLOPs - Applying rotary position embeddings to Q and K
    QK^T Multiplication:   3,655,680 FLOPs - Computing attention scores via Q*K^T
    Attention Maps:        14,336 FLOPs - Adding attention maps across heads
    Scaling:               14,476 FLOPs - Dividing by sqrt(attention_dim)
    Softmax:               171,584 FLOPs - Computing softmax for attention weights
    Attention*V:           3,655,680 FLOPs - Applying attention weights to values
    Output Projection:     102,731,776 FLOPs - Projecting attention outputs back to hidden dimension
    ---
    Total Attention FLOPs: 418,639,564

Total Attention FLOPS: 418,639,564
</pre></div></div>
</div>
</section>
<section id="2.-RMSNorm-FLOPS">
<h2>2. RMSNorm FLOPS<a class="headerlink" href="#2.-RMSNorm-FLOPS" title="Link to this heading"></a></h2>
<p>Next, we’ll look at RMSNorm (Root Mean Square Normalization) used in Qwen2.5 instead of LayerNorm. The calculation includes:</p>
<ol class="arabic simple">
<li><p>Calculating variance (squared mean)</p></li>
<li><p>Applying normalization factor (1/sqrt(variance + epsilon))</p></li>
<li><p>Scaling with learnable weights</p></li>
</ol>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rmsnorm_flops</span><span class="p">,</span> <span class="n">rmsnorm_breakdown</span> <span class="o">=</span> <span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_RMSNorm_flops</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">verbose</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">rmsnorm_breakdown</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total RMSNorm FLOPS: </span><span class="si">{</span><span class="n">rmsnorm_flops</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
RMSNorm FLOPs Breakdown:
    Variance Calculation:  57,344 FLOPs - Computing squared mean for normalization
    Normalization:         29,056 FLOPs - Applying normalization factor to hidden states
    Weight Application:    28,672 FLOPs - Applying learnable weight parameters
    ---
    Total RMSNorm FLOPs:   115,072

Total RMSNorm FLOPS: 115,072
</pre></div></div>
</div>
</section>
<section id="3.-Feed-Forward-Network-FLOPS">
<h2>3. Feed-Forward Network FLOPS<a class="headerlink" href="#3.-Feed-Forward-Network-FLOPS" title="Link to this heading"></a></h2>
<p>The FFN in Qwen2.5 uses a SwiGLU activation and includes:</p>
<ol class="arabic simple">
<li><p>Gate projection with SiLU activation</p></li>
<li><p>Up projection</p></li>
<li><p>Element-wise multiplication of gate and up projections</p></li>
<li><p>Down projection</p></li>
</ol>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ffn_flops</span><span class="p">,</span> <span class="n">ffn_breakdown</span> <span class="o">=</span> <span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_ffn_flops</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">verbose</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ffn_breakdown</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total FFN FLOPS: </span><span class="si">{</span><span class="n">ffn_flops</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
FFN FLOPs Breakdown:
    Gate Projection:       280,944,640 FLOPs - Projecting to gating dimension with SiLU activation
    Up Projection:         278,765,568 FLOPs - Projecting to intermediate dimension
    Gated Multiplication:  155,648 FLOPs - Element-wise multiplication of gate and up projections
    Down Projection:       278,892,544 FLOPs - Projecting back to hidden dimension
    ---
    Total FFN FLOPs:       838,758,400

Total FFN FLOPS: 838,758,400
</pre></div></div>
</div>
</section>
<section id="4.-LoRA-Adaptation-FLOPS">
<h2>4. LoRA Adaptation FLOPS<a class="headerlink" href="#4.-LoRA-Adaptation-FLOPS" title="Link to this heading"></a></h2>
<p>For our fine-tuning, we use LoRA adaptations which add computations:</p>
<ol class="arabic simple">
<li><p>Down-projection to low-rank space</p></li>
<li><p>Up-projection to original dimensionality</p></li>
<li><p>Scaling and addition to original weights</p></li>
</ol>
<p>Let’s see the FLOPS for LoRA with rank=4:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lora_rank</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">lora_flops</span><span class="p">,</span> <span class="n">lora_breakdown</span> <span class="o">=</span> <span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_lora_flops</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">lora_rank</span><span class="p">,</span> <span class="n">verbose</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">lora_breakdown</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total LoRA FLOPS (per layer where applied): </span><span class="si">{</span><span class="n">lora_flops</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
LoRA FLOPs Breakdown:
    Down Projection:       229,248 FLOPs - Projecting to low-rank dimension
    Up Projection:         28,672 FLOPs - Projecting from low-rank to attention dimension
    Scaling Coefficient:   4,097 FLOPs - Scaling coefficient for LoRA output
    Addition to Output:    4,096 FLOPs - Adding LoRA output to original output
    ---
    Total LoRA FLOPs:      266,113

Total LoRA FLOPS (per layer where applied): 266,113
</pre></div></div>
</div>
</section>
<section id="5.-Language-Model-Head-FLOPS">
<h2>5. Language Model Head FLOPS<a class="headerlink" href="#5.-Language-Model-Head-FLOPS" title="Link to this heading"></a></h2>
<p>The language model head projects from hidden states to vocabulary size:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lm_head_flops</span><span class="p">,</span> <span class="n">lm_head_breakdown</span> <span class="o">=</span> <span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_LM_head</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">verbose</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">lm_head_breakdown</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total LM Head FLOPS: </span><span class="si">{</span><span class="n">lm_head_flops</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
LM Head FLOPs Breakdown:
    LM Head Projection:    272,269,312 FLOPs - Projecting hidden states to vocabulary size
    ---
    Total LM Head FLOPs:   272,269,312

Total LM Head FLOPS: 272,269,312
</pre></div></div>
</div>
</section>
<section id="6.-Loss-Calculation-FLOPS">
<h2>6. Loss Calculation FLOPS<a class="headerlink" href="#6.-Loss-Calculation-FLOPS" title="Link to this heading"></a></h2>
<p>For training, we need to compute the loss:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loss_flops</span><span class="p">,</span> <span class="n">loss_breakdown</span> <span class="o">=</span> <span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_loss_flops</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">verbose</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">loss_breakdown</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total Loss Calculation FLOPS: </span><span class="si">{</span><span class="n">loss_flops</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Loss FLOPs Breakdown:
    Softmax:               58,343,424 FLOPs - Computing softmax for loss calculation
    Log and Time:          352 FLOPs - Computing log and multiply by true label
    ---
    Total Loss FLOPs:      58,343,776

Total Loss Calculation FLOPS: 58,343,776
</pre></div></div>
</div>
</section>
<section id="7.-Residual-Connection-FLOPS">
<h2>7. Residual Connection FLOPS<a class="headerlink" href="#7.-Residual-Connection-FLOPS" title="Link to this heading"></a></h2>
<p>Simple residual connections that add the input to the output:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">residual_flops</span><span class="p">,</span> <span class="n">residual_breakdown</span> <span class="o">=</span> <span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_residual_flops</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">verbose</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">residual_breakdown</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total Residual Connection FLOPS: </span><span class="si">{</span><span class="n">residual_flops</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Residual FLOPs Breakdown:
    Residual Addition:     28,672 FLOPs - Adding residual connection to layer output
    ---
    Total Residual FLOPs:  28,672

Total Residual Connection FLOPS: 28,672
</pre></div></div>
</div>
</section>
<section id="Full-Model-FLOPS-Breakdown">
<h2>Full Model FLOPS Breakdown<a class="headerlink" href="#Full-Model-FLOPS-Breakdown" title="Link to this heading"></a></h2>
<p>Now let’s see the full model FLOPS calculation, including both training and inference:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Training FLOPS</span>
<span class="n">training_flops</span> <span class="o">=</span> <span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_flops</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">lora_rank</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inference</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Total Training FLOPS: </span><span class="si">{</span><span class="n">training_flops</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Inference FLOPS</span>
<span class="n">inference_flops</span> <span class="o">=</span> <span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_flops</span><span class="p">(</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">lora_rank</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inference</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Total Inference FLOPS: </span><span class="si">{</span><span class="n">inference_flops</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Training/Inference FLOPS ratio: </span><span class="si">{</span><span class="n">training_flops</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">inference_flops</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">x&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

=== QWEN 2.5 FLOPs Calculation ===

==== Component Breakdown ====
Attention FLOPs Breakdown:
    QKV Projection:        308,281,344 FLOPs - Matrix multiplication to project embeddings to QKV
    Rotary Embedding:      114,688 FLOPs - Applying rotary position embeddings to Q and K
    QK^T Multiplication:   3,655,680 FLOPs - Computing attention scores via Q*K^T
    Attention Maps:        14,336 FLOPs - Adding attention maps across heads
    Scaling:               14,476 FLOPs - Dividing by sqrt(attention_dim)
    Softmax:               171,584 FLOPs - Computing softmax for attention weights
    Attention*V:           3,655,680 FLOPs - Applying attention weights to values
    Output Projection:     102,731,776 FLOPs - Projecting attention outputs back to hidden dimension
    ---
    Total Attention FLOPs: 418,639,564

RMSNorm FLOPs Breakdown:
    Variance Calculation:  57,344 FLOPs - Computing squared mean for normalization
    Normalization:         29,056 FLOPs - Applying normalization factor to hidden states
    Weight Application:    28,672 FLOPs - Applying learnable weight parameters
    ---
    Total RMSNorm FLOPs:   115,072

FFN FLOPs Breakdown:
    Gate Projection:       280,944,640 FLOPs - Projecting to gating dimension with SiLU activation
    Up Projection:         278,765,568 FLOPs - Projecting to intermediate dimension
    Gated Multiplication:  155,648 FLOPs - Element-wise multiplication of gate and up projections
    Down Projection:       278,892,544 FLOPs - Projecting back to hidden dimension
    ---
    Total FFN FLOPs:       838,758,400

LoRA FLOPs Breakdown:
    Down Projection:       229,248 FLOPs - Projecting to low-rank dimension
    Up Projection:         28,672 FLOPs - Projecting from low-rank to attention dimension
    Scaling Coefficient:   4,097 FLOPs - Scaling coefficient for LoRA output
    Addition to Output:    4,096 FLOPs - Adding LoRA output to original output
    ---
    Total LoRA FLOPs:      266,113

Residual FLOPs Breakdown:
    Residual Addition:     28,672 FLOPs - Adding residual connection to layer output
    ---
    Total Residual FLOPs:  28,672


==== Decoder Layer Breakdown ====
FLOPs per Decoder Layer:
    2 × RMSNorm:           230,144 FLOPs
    2 × Residual:          57,344 FLOPs
    1 × Attention:         418,639,564 FLOPs
    1 × FFN:               838,758,400 FLOPs
    28 × LoRA:              7,451,164 FLOPs (LoRA for Q and K in each head, and only present when training, not in inference)
    To Logits:             272,269,312 FLOPs
    ---
    Total per Layer:       1,257,685,452 FLOPs

==== Full Model FLOPs ====
Forward Pass (24 layers):        30,456,720,160 FLOPs
Backward Pass (≈2× forward): 61,110,825,140 FLOPs
---
Total Training FLOPs:      91,567,545,300 FLOPs


Total Training FLOPS: 91,567,545,300

Total Inference FLOPS: 30,456,720,160
Training/Inference FLOPS ratio: 3.01x
</pre></div></div>
</div>
</section>
<section id="FLOPS-Scaling-with-Different-Parameters">
<h2>FLOPS Scaling with Different Parameters<a class="headerlink" href="#FLOPS-Scaling-with-Different-Parameters" title="Link to this heading"></a></h2>
<p>Let’s analyze how FLOPS scale with different parameters:</p>
<ol class="arabic simple">
<li><p>Sequence length</p></li>
<li><p>Batch size</p></li>
<li><p>LoRA rank</p></li>
</ol>
<p>For better visualization, the FLOPS are log-scaled. To interpret the values on the y-axis, note that the FLOPS value is <span class="math notranslate nohighlight">\(10^{y_{value}}\)</span>. This scaling helps to visualize how close each operation is to our budget of <span class="math notranslate nohighlight">\(10^{17}\)</span> FLOPS.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Sequence length scaling</span>
<span class="n">seq_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">768</span><span class="p">]</span>
<span class="n">train_flops_by_seq</span> <span class="o">=</span> <span class="p">[</span><span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_flops</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">inference</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">sl</span> <span class="ow">in</span> <span class="n">seq_lengths</span><span class="p">]</span>
<span class="n">infer_flops_by_seq</span> <span class="o">=</span> <span class="p">[</span><span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_flops</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">sl</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">inference</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">sl</span> <span class="ow">in</span> <span class="n">seq_lengths</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">train_flops_by_seq</span><span class="p">],</span> <span class="s1">&#39;b-o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">seq_lengths</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">infer_flops_by_seq</span><span class="p">],</span> <span class="s1">&#39;r-o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Inference&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sequence Length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;FLOPS (Log10)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;FLOPS vs. Sequence Length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Batch size scaling</span>
<span class="n">batch_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">]</span>
<span class="n">train_flops_by_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_flops</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">inference</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">bs</span> <span class="ow">in</span> <span class="n">batch_sizes</span><span class="p">]</span>
<span class="n">infer_flops_by_batch</span> <span class="o">=</span> <span class="p">[</span><span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_flops</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">inference</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">for</span> <span class="n">bs</span> <span class="ow">in</span> <span class="n">batch_sizes</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">batch_sizes</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">train_flops_by_batch</span><span class="p">],</span> <span class="s1">&#39;b-o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Training&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">batch_sizes</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">infer_flops_by_batch</span><span class="p">],</span> <span class="s1">&#39;r-o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Inference&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Batch Size&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;FLOPS (Log10)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;FLOPS vs. Batch Size&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># LoRA rank scaling</span>
<span class="n">lora_ranks</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>
<span class="n">train_flops_by_rank</span> <span class="o">=</span> <span class="p">[</span><span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_flops</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">inference</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">lora_ranks</span><span class="p">]</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lora_ranks</span><span class="p">,</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">train_flops_by_rank</span><span class="p">],</span> <span class="s1">&#39;b-o&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;LoRA Rank&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;FLOPS (Log10)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Training FLOPS vs. LoRA Rank&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_2_flops_calculation_21_0.png" src="../_images/notebooks_2_flops_calculation_21_0.png" />
</div>
</div>
</section>
<section id="Budget-Planning-Analysis">
<h2>Budget Planning Analysis<a class="headerlink" href="#Budget-Planning-Analysis" title="Link to this heading"></a></h2>
<p>Given our 10^17 FLOPS budget, let’s analyze how many training iterations we can afford for different configurations:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pandas.io.formats.style</span><span class="w"> </span><span class="kn">import</span> <span class="n">Styler</span>

<span class="c1"># Create a grid of configurations</span>
<span class="n">configs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">seq_len</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">768</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">batch_size</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]:</span>
        <span class="k">for</span> <span class="n">lora_rank</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]:</span>
            <span class="n">flops</span> <span class="o">=</span> <span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_flops</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">lora_rank</span><span class="p">,</span> <span class="n">inference</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="n">iterations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e17</span> <span class="o">/</span> <span class="n">flops</span><span class="p">)</span>  <span class="c1"># How many iterations fit in our budget</span>
            <span class="n">configs</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
                <span class="s2">&quot;seq_len&quot;</span><span class="p">:</span> <span class="n">seq_len</span><span class="p">,</span>
                <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="n">batch_size</span><span class="p">,</span>
                <span class="s2">&quot;lora_rank&quot;</span><span class="p">:</span> <span class="n">lora_rank</span><span class="p">,</span>
                <span class="s2">&quot;flops_per_iter&quot;</span><span class="p">:</span> <span class="n">flops</span><span class="p">,</span>
                <span class="s2">&quot;max_iterations&quot;</span><span class="p">:</span> <span class="n">iterations</span><span class="p">,</span>
                <span class="s2">&quot;tokens_processed&quot;</span><span class="p">:</span> <span class="n">iterations</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">seq_len</span>
            <span class="p">})</span>

<span class="c1"># Convert to DataFrame for easier analysis</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">configs</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;tokens_processed&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Create a more informative interactive visualization</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">18</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># 1. Iterations vs Sequence Length by Batch Size</span>
<span class="k">for</span> <span class="n">bs</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]:</span>
    <span class="n">subset</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">bs</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]:</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">subset</span><span class="p">[</span><span class="n">subset</span><span class="p">[</span><span class="s2">&quot;lora_rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">rank</span><span class="p">][</span><span class="s2">&quot;seq_len&quot;</span><span class="p">],</span>
                    <span class="n">subset</span><span class="p">[</span><span class="n">subset</span><span class="p">[</span><span class="s2">&quot;lora_rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">rank</span><span class="p">][</span><span class="s2">&quot;max_iterations&quot;</span><span class="p">],</span>
                    <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;BS=</span><span class="si">{</span><span class="n">bs</span><span class="si">}</span><span class="s2">, Rank=</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Sequence Length&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Max Iterations&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Max Iterations vs. Sequence Length&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>


<span class="c1"># 2. Iterations vs Batch Size by Sequence Length</span>
<span class="k">for</span> <span class="n">sl</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">768</span><span class="p">]:</span>
    <span class="n">subset</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;seq_len&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">sl</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">rank</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]:</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">subset</span><span class="p">[</span><span class="n">subset</span><span class="p">[</span><span class="s2">&quot;lora_rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">rank</span><span class="p">][</span><span class="s2">&quot;batch_size&quot;</span><span class="p">],</span>
                    <span class="n">subset</span><span class="p">[</span><span class="n">subset</span><span class="p">[</span><span class="s2">&quot;lora_rank&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">rank</span><span class="p">][</span><span class="s2">&quot;max_iterations&quot;</span><span class="p">],</span>
                    <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Seq=</span><span class="si">{</span><span class="n">sl</span><span class="si">}</span><span class="s2">, Rank=</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Batch Size&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Max Iterations&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Max Iterations vs. Batch Size&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>

<span class="c1"># 3. Iterations vs LoRA Rank by Sequence Length</span>
<span class="k">for</span> <span class="n">sl</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">768</span><span class="p">]:</span>
    <span class="n">subset</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;seq_len&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">sl</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">bs</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">]:</span>
        <span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">subset</span><span class="p">[</span><span class="n">subset</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">bs</span><span class="p">][</span><span class="s2">&quot;lora_rank&quot;</span><span class="p">],</span>
                    <span class="n">subset</span><span class="p">[</span><span class="n">subset</span><span class="p">[</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">bs</span><span class="p">][</span><span class="s2">&quot;max_iterations&quot;</span><span class="p">],</span>
                    <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Seq=</span><span class="si">{</span><span class="n">sl</span><span class="si">}</span><span class="s2">, BS=</span><span class="si">{</span><span class="n">bs</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;LoRA Rank&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Max Iterations&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Max Iterations vs. LoRA Rank&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>

<span class="c1">#</span>

<span class="c1"># Create an interactive table showing the tradeoff between parameters</span>

<span class="c1"># Highlight the table based on max_iterations</span>
<span class="k">def</span><span class="w"> </span><span class="nf">highlight_max_iterations</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="n">is_max</span> <span class="o">=</span> <span class="n">s</span> <span class="o">==</span> <span class="n">s</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="k">return</span> <span class="p">[</span><span class="s1">&#39;background-color: yellow&#39;</span> <span class="k">if</span> <span class="n">v</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">is_max</span><span class="p">]</span>

<span class="n">styled_df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">highlight_max_iterations</span><span class="p">,</span> <span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;max_iterations&#39;</span><span class="p">])</span>
<span class="n">styled_df</span> <span class="o">=</span> <span class="n">styled_df</span><span class="o">.</span><span class="n">format</span><span class="p">({</span>
    <span class="s1">&#39;flops_per_iter&#39;</span><span class="p">:</span> <span class="s1">&#39;</span><span class="si">{:,.0f}</span><span class="s1">&#39;</span><span class="p">,</span>
    <span class="s1">&#39;max_iterations&#39;</span><span class="p">:</span> <span class="s1">&#39;</span><span class="si">{:,.0f}</span><span class="s1">&#39;</span><span class="p">,</span>
<span class="p">})</span>

<span class="n">display</span><span class="p">(</span><span class="n">HTML</span><span class="p">(</span><span class="s2">&quot;&lt;h3&gt;Configuration Trade-offs (sorted by max iterations)&lt;/h3&gt;&quot;</span><span class="p">))</span>
<span class="n">display</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;max_iterations&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">15</span><span class="p">)</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">format</span><span class="p">({</span>
    <span class="s1">&#39;flops_per_iter&#39;</span><span class="p">:</span> <span class="s1">&#39;</span><span class="si">{:,.0f}</span><span class="s1">&#39;</span><span class="p">,</span>
    <span class="s1">&#39;max_iterations&#39;</span><span class="p">:</span> <span class="s1">&#39;</span><span class="si">{:,.0f}</span><span class="s1">&#39;</span><span class="p">,</span>
<span class="p">}))</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<h3>Configuration Trade-offs (sorted by max iterations)</h3></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<style type="text/css">
</style>
<table id="T_b6005">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_b6005_level0_col0" class="col_heading level0 col0" >seq_len</th>
      <th id="T_b6005_level0_col1" class="col_heading level0 col1" >batch_size</th>
      <th id="T_b6005_level0_col2" class="col_heading level0 col2" >lora_rank</th>
      <th id="T_b6005_level0_col3" class="col_heading level0 col3" >flops_per_iter</th>
      <th id="T_b6005_level0_col4" class="col_heading level0 col4" >max_iterations</th>
      <th id="T_b6005_level0_col5" class="col_heading level0 col5" >tokens_processed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_b6005_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_b6005_row0_col0" class="data row0 col0" >128</td>
      <td id="T_b6005_row0_col1" class="data row0 col1" >1</td>
      <td id="T_b6005_row0_col2" class="data row0 col2" >2</td>
      <td id="T_b6005_row0_col3" class="data row0 col3" >370,266,131,508</td>
      <td id="T_b6005_row0_col4" class="data row0 col4" >270,076</td>
      <td id="T_b6005_row0_col5" class="data row0 col5" >34569728</td>
    </tr>
    <tr>
      <th id="T_b6005_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_b6005_row1_col0" class="data row1 col0" >128</td>
      <td id="T_b6005_row1_col1" class="data row1 col1" >1</td>
      <td id="T_b6005_row1_col2" class="data row1 col2" >4</td>
      <td id="T_b6005_row1_col3" class="data row1 col3" >370,310,150,196</td>
      <td id="T_b6005_row1_col4" class="data row1 col4" >270,043</td>
      <td id="T_b6005_row1_col5" class="data row1 col5" >34565504</td>
    </tr>
    <tr>
      <th id="T_b6005_level0_row2" class="row_heading level0 row2" >2</th>
      <td id="T_b6005_row2_col0" class="data row2 col0" >128</td>
      <td id="T_b6005_row2_col1" class="data row2 col1" >1</td>
      <td id="T_b6005_row2_col2" class="data row2 col2" >8</td>
      <td id="T_b6005_row2_col3" class="data row2 col3" >370,398,187,572</td>
      <td id="T_b6005_row2_col4" class="data row2 col4" >269,979</td>
      <td id="T_b6005_row2_col5" class="data row2 col5" >34557312</td>
    </tr>
    <tr>
      <th id="T_b6005_level0_row3" class="row_heading level0 row3" >3</th>
      <td id="T_b6005_row3_col0" class="data row3 col0" >128</td>
      <td id="T_b6005_row3_col1" class="data row3 col1" >2</td>
      <td id="T_b6005_row3_col2" class="data row3 col2" >2</td>
      <td id="T_b6005_row3_col3" class="data row3 col3" >740,532,262,932</td>
      <td id="T_b6005_row3_col4" class="data row3 col4" >135,038</td>
      <td id="T_b6005_row3_col5" class="data row3 col5" >34569728</td>
    </tr>
    <tr>
      <th id="T_b6005_level0_row4" class="row_heading level0 row4" >4</th>
      <td id="T_b6005_row4_col0" class="data row4 col0" >128</td>
      <td id="T_b6005_row4_col1" class="data row4 col1" >2</td>
      <td id="T_b6005_row4_col2" class="data row4 col2" >4</td>
      <td id="T_b6005_row4_col3" class="data row4 col3" >740,620,300,308</td>
      <td id="T_b6005_row4_col4" class="data row4 col4" >135,021</td>
      <td id="T_b6005_row4_col5" class="data row4 col5" >34565376</td>
    </tr>
    <tr>
      <th id="T_b6005_level0_row5" class="row_heading level0 row5" >5</th>
      <td id="T_b6005_row5_col0" class="data row5 col0" >128</td>
      <td id="T_b6005_row5_col1" class="data row5 col1" >2</td>
      <td id="T_b6005_row5_col2" class="data row5 col2" >8</td>
      <td id="T_b6005_row5_col3" class="data row5 col3" >740,796,375,060</td>
      <td id="T_b6005_row5_col4" class="data row5 col4" >134,989</td>
      <td id="T_b6005_row5_col5" class="data row5 col5" >34557184</td>
    </tr>
    <tr>
      <th id="T_b6005_level0_row6" class="row_heading level0 row6" >6</th>
      <td id="T_b6005_row6_col0" class="data row6 col0" >128</td>
      <td id="T_b6005_row6_col1" class="data row6 col1" >4</td>
      <td id="T_b6005_row6_col2" class="data row6 col2" >2</td>
      <td id="T_b6005_row6_col3" class="data row6 col3" >1,481,064,525,780</td>
      <td id="T_b6005_row6_col4" class="data row6 col4" >67,519</td>
      <td id="T_b6005_row6_col5" class="data row6 col5" >34569728</td>
    </tr>
    <tr>
      <th id="T_b6005_level0_row7" class="row_heading level0 row7" >7</th>
      <td id="T_b6005_row7_col0" class="data row7 col0" >128</td>
      <td id="T_b6005_row7_col1" class="data row7 col1" >4</td>
      <td id="T_b6005_row7_col2" class="data row7 col2" >4</td>
      <td id="T_b6005_row7_col3" class="data row7 col3" >1,481,240,600,532</td>
      <td id="T_b6005_row7_col4" class="data row7 col4" >67,510</td>
      <td id="T_b6005_row7_col5" class="data row7 col5" >34565120</td>
    </tr>
    <tr>
      <th id="T_b6005_level0_row8" class="row_heading level0 row8" >8</th>
      <td id="T_b6005_row8_col0" class="data row8 col0" >128</td>
      <td id="T_b6005_row8_col1" class="data row8 col1" >4</td>
      <td id="T_b6005_row8_col2" class="data row8 col2" >8</td>
      <td id="T_b6005_row8_col3" class="data row8 col3" >1,481,592,750,036</td>
      <td id="T_b6005_row8_col4" class="data row8 col4" >67,494</td>
      <td id="T_b6005_row8_col5" class="data row8 col5" >34556928</td>
    </tr>
    <tr>
      <th id="T_b6005_level0_row9" class="row_heading level0 row9" >12</th>
      <td id="T_b6005_row9_col0" class="data row9 col0" >512</td>
      <td id="T_b6005_row9_col1" class="data row9 col1" >1</td>
      <td id="T_b6005_row9_col2" class="data row9 col2" >2</td>
      <td id="T_b6005_row9_col3" class="data row9 col3" >1,582,460,844,468</td>
      <td id="T_b6005_row9_col4" class="data row9 col4" >63,192</td>
      <td id="T_b6005_row9_col5" class="data row9 col5" >32354304</td>
    </tr>
    <tr>
      <th id="T_b6005_level0_row10" class="row_heading level0 row10" >13</th>
      <td id="T_b6005_row10_col0" class="data row10 col0" >512</td>
      <td id="T_b6005_row10_col1" class="data row10 col1" >1</td>
      <td id="T_b6005_row10_col2" class="data row10 col2" >4</td>
      <td id="T_b6005_row10_col3" class="data row10 col3" >1,582,636,919,220</td>
      <td id="T_b6005_row10_col4" class="data row10 col4" >63,185</td>
      <td id="T_b6005_row10_col5" class="data row10 col5" >32350720</td>
    </tr>
    <tr>
      <th id="T_b6005_level0_row11" class="row_heading level0 row11" >14</th>
      <td id="T_b6005_row11_col0" class="data row11 col0" >512</td>
      <td id="T_b6005_row11_col1" class="data row11 col1" >1</td>
      <td id="T_b6005_row11_col2" class="data row11 col2" >8</td>
      <td id="T_b6005_row11_col3" class="data row11 col3" >1,582,989,068,724</td>
      <td id="T_b6005_row11_col4" class="data row11 col4" >63,171</td>
      <td id="T_b6005_row11_col5" class="data row11 col5" >32343552</td>
    </tr>
    <tr>
      <th id="T_b6005_level0_row12" class="row_heading level0 row12" >24</th>
      <td id="T_b6005_row12_col0" class="data row12 col0" >768</td>
      <td id="T_b6005_row12_col1" class="data row12 col1" >1</td>
      <td id="T_b6005_row12_col2" class="data row12 col2" >2</td>
      <td id="T_b6005_row12_col3" class="data row12 col3" >2,477,129,630,388</td>
      <td id="T_b6005_row12_col4" class="data row12 col4" >40,369</td>
      <td id="T_b6005_row12_col5" class="data row12 col5" >31003392</td>
    </tr>
    <tr>
      <th id="T_b6005_level0_row13" class="row_heading level0 row13" >25</th>
      <td id="T_b6005_row13_col0" class="data row13 col0" >768</td>
      <td id="T_b6005_row13_col1" class="data row13 col1" >1</td>
      <td id="T_b6005_row13_col2" class="data row13 col2" >4</td>
      <td id="T_b6005_row13_col3" class="data row13 col3" >2,477,393,742,516</td>
      <td id="T_b6005_row13_col4" class="data row13 col4" >40,365</td>
      <td id="T_b6005_row13_col5" class="data row13 col5" >31000320</td>
    </tr>
    <tr>
      <th id="T_b6005_level0_row14" class="row_heading level0 row14" >26</th>
      <td id="T_b6005_row14_col0" class="data row14 col0" >768</td>
      <td id="T_b6005_row14_col1" class="data row14 col1" >1</td>
      <td id="T_b6005_row14_col2" class="data row14 col2" >8</td>
      <td id="T_b6005_row14_col3" class="data row14 col3" >2,477,921,966,772</td>
      <td id="T_b6005_row14_col4" class="data row14 col4" >40,356</td>
      <td id="T_b6005_row14_col5" class="data row14 col5" >30993408</td>
    </tr>
  </tbody>
</table></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_2_flops_calculation_23_2.png" src="../_images/notebooks_2_flops_calculation_23_2.png" />
</div>
</div>
<p>Now for the inference FLOPS calculation, we can use the same formula as the training FLOPS calculation, but with the following modifications:</p>
<ul class="simple">
<li><p><strong>no LoRA Flops</strong>, since LoRA can be merged back into the model weights before inference, and the model weights can be used directly for inference</p></li>
<li><p><strong>no loss function calculation Flops</strong>, since we don’t need to calculate the loss during inference</p></li>
<li><p><strong>no gradient calculation Flops</strong>, since we don’t need to calculate gradients during inference</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">IPython.display</span><span class="w"> </span><span class="kn">import</span> <span class="n">HTML</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="c1"># Create configurations for inference at selected sequence lengths</span>
<span class="n">seq_lengths</span> <span class="o">=</span> <span class="p">[</span><span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">768</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Fixed batch size of 1</span>

<span class="c1"># Create configurations for inference</span>
<span class="n">configs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">seq_len</span> <span class="ow">in</span> <span class="n">seq_lengths</span><span class="p">:</span>
    <span class="c1"># For inference, we don&#39;t use LoRA as it can be merged with weights</span>
    <span class="n">flops</span> <span class="o">=</span> <span class="n">flops_calculator</span><span class="o">.</span><span class="n">get_flops</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">inference</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">iterations</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">1e17</span> <span class="o">/</span> <span class="n">flops</span><span class="p">)</span>  <span class="c1"># How many iterations fit in our budget</span>
    <span class="n">configs</span><span class="o">.</span><span class="n">append</span><span class="p">({</span>
        <span class="s2">&quot;seq_len&quot;</span><span class="p">:</span> <span class="n">seq_len</span><span class="p">,</span>
        <span class="s2">&quot;flops_per_iter&quot;</span><span class="p">:</span> <span class="n">flops</span><span class="p">,</span>
        <span class="s2">&quot;max_iterations&quot;</span><span class="p">:</span> <span class="n">iterations</span><span class="p">,</span>
        <span class="s2">&quot;tokens_processed&quot;</span><span class="p">:</span> <span class="n">iterations</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">seq_len</span>
    <span class="p">})</span>

<span class="c1"># Convert to DataFrame for analysis</span>
<span class="n">df_inference</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">configs</span><span class="p">)</span>
<span class="n">df_inference</span> <span class="o">=</span> <span class="n">df_inference</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;tokens_processed&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Create a visualization of inference FLOPS scaling</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="c1"># Plot 1: FLOPS vs Sequence Length</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_inference</span><span class="p">[</span><span class="s2">&quot;seq_len&quot;</span><span class="p">],</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">f</span><span class="p">)</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">df_inference</span><span class="p">[</span><span class="s2">&quot;flops_per_iter&quot;</span><span class="p">]],</span>
         <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sequence Length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;FLOPS (Log10)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Inference FLOPS vs. Sequence Length (Batch Size=1)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Plot 2: Max Iterations vs Sequence Length</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df_inference</span><span class="p">[</span><span class="s2">&quot;seq_len&quot;</span><span class="p">],</span> <span class="n">df_inference</span><span class="p">[</span><span class="s2">&quot;max_iterations&quot;</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sequence Length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Max Iterations (within 10^17 budget)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Max Inference Iterations vs. Sequence Length (Batch Size=1)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Display table of configurations</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Inference configurations with batch size=1:&quot;</span><span class="p">)</span>
<span class="n">display</span><span class="p">(</span><span class="n">df_inference</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">format</span><span class="p">({</span>
    <span class="s1">&#39;flops_per_iter&#39;</span><span class="p">:</span> <span class="s1">&#39;</span><span class="si">{:,.0f}</span><span class="s1">&#39;</span><span class="p">,</span>
    <span class="s1">&#39;max_iterations&#39;</span><span class="p">:</span> <span class="s1">&#39;</span><span class="si">{:,.0f}</span><span class="s1">&#39;</span><span class="p">,</span>
<span class="p">}))</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_2_flops_calculation_25_0.png" src="../_images/notebooks_2_flops_calculation_25_0.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

Inference configurations with batch size=1:
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<style type="text/css">
</style>
<table id="T_44fc9">
  <thead>
    <tr>
      <th class="blank level0" >&nbsp;</th>
      <th id="T_44fc9_level0_col0" class="col_heading level0 col0" >seq_len</th>
      <th id="T_44fc9_level0_col1" class="col_heading level0 col1" >flops_per_iter</th>
      <th id="T_44fc9_level0_col2" class="col_heading level0 col2" >max_iterations</th>
      <th id="T_44fc9_level0_col3" class="col_heading level0 col3" >tokens_processed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th id="T_44fc9_level0_row0" class="row_heading level0 row0" >0</th>
      <td id="T_44fc9_row0_col0" class="data row0 col0" >128</td>
      <td id="T_44fc9_row0_col1" class="data row0 col1" >123,173,537,056</td>
      <td id="T_44fc9_row0_col2" class="data row0 col2" >811,862</td>
      <td id="T_44fc9_row0_col3" class="data row0 col3" >103918336</td>
    </tr>
    <tr>
      <th id="T_44fc9_level0_row1" class="row_heading level0 row1" >1</th>
      <td id="T_44fc9_row1_col0" class="data row1 col0" >512</td>
      <td id="T_44fc9_row1_col1" class="data row1 col1" >526,492,921,120</td>
      <td id="T_44fc9_row1_col2" class="data row1 col2" >189,936</td>
      <td id="T_44fc9_row1_col3" class="data row1 col3" >97247232</td>
    </tr>
    <tr>
      <th id="T_44fc9_level0_row2" class="row_heading level0 row2" >2</th>
      <td id="T_44fc9_row2_col0" class="data row2 col0" >768</td>
      <td id="T_44fc9_row2_col1" class="data row2 col1" >824,218,836,256</td>
      <td id="T_44fc9_row2_col2" class="data row2 col2" >121,327</td>
      <td id="T_44fc9_row2_col3" class="data row2 col3" >93179136</td>
    </tr>
  </tbody>
</table></div>
</div>
<p>Now, give some stats on the <code class="docutils literal notranslate"><span class="pre">flops_per_iter</span></code> and <code class="docutils literal notranslate"><span class="pre">max_iterations</span></code> for the inference and training FLOPs calculation, I have gotten a sense of how many iterations we can afford for different configurations. Thus helping us to plan our budget effectively.</p>
</section>
<section id="Conclusion">
<h2>Conclusion<a class="headerlink" href="#Conclusion" title="Link to this heading"></a></h2>
<p>This notebook provided a detailed explanation of how we calculate FLOPS for the Qwen2.5-0.5B model with LoRA fine-tuning. The key insights are:</p>
<ol class="arabic simple">
<li><p><strong>Component-wise breakdown</strong>: We analyzed FLOPS for each architectural component</p></li>
<li><p><strong>Scaling behavior</strong>: We showed how FLOPS scale with sequence length, batch size, and LoRA rank</p></li>
<li><p><strong>Budget planning</strong>: We examined how to optimize configurations within our FLOPS budget</p></li>
<li><p><strong>Experiment accounting</strong>: We demonstrated how to track FLOPS for complete experiments</p></li>
</ol>
<p>This understanding allows us to:</p>
<ol class="arabic simple">
<li><p>Plan experiments efficiently within our computational budget</p></li>
<li><p>Make informed trade-offs between hyperparameters</p></li>
<li><p>Report accurate FLOPS usage as required by the coursework</p></li>
</ol>
<p>The implementation in <code class="docutils literal notranslate"><span class="pre">get_flops.py</span></code> enables us to track and log FLOPS throughout our training and evaluation processes.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="1_dataset_preprocess.html" class="btn btn-neutral float-left" title="Lotka-Volterra Dataset Exploration &amp; LLMTIME Preprocessing" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="3_untrained_behaviour.html" class="btn btn-neutral float-right" title="Evaluating Untrained LLM Performance on Time Series Forecasting" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Yuchen Mao.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>