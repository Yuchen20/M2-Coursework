{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c296c4c5-3eb1-4c1a-ad5a-46a409d55951",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Train LoRA-LLM\n",
    "This is a tutorial notebook on how to train a Qwen model with LoRA using our `Trainer` class. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e8e572-81a8-463e-a5a8-f83c5bc1461a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__name__), os.path.pardir)))\n",
    "from src.get_flops import QwenFlopsCalculator\n",
    "from src.get_data import LotkaVolterraDataset, DataMaster\n",
    "from src.preprocessor import NumericalProcessor\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__name__), os.path.pardir)))\n",
    "\n",
    "from src.Trainer import LoRATrainer\n",
    "\n",
    "# models\n",
    "def load_qwen():\n",
    "    model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "    # Freeze all parameters except LM head bias\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Add trainable bias to logits\n",
    "    assert model.lm_head.bias is None\n",
    "    model.lm_head.bias = torch.nn.Parameter(\n",
    "        torch.zeros(model.config.vocab_size, device=model.device)\n",
    "    )\n",
    "    model.lm_head.bias.requires_grad = True\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# some nice function for GPU Training\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "    with torch.device('cuda'):\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved LoRA Training\n",
    "Here we have the LoRALinear class with a slight modification to allow for the merging of LoRA into the original model. A key benefit of LoRA compared to other parameter-efficient tuning methods is that it allows for the merging of the LoRA weights into the original model weights. This means that after training, the model can be used without the LoRA adapter, which can save memory and improve inference speed. The merging process involves adding the LoRA weights to the original model weights in a way that preserves the original model's performance while also incorporating the new knowledge learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e28eb48-f876-4151-ad01-00c786c42861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here, we showcase few of the trainable parameters: ['model.layers.0.self_attn.q_proj.A', 'model.layers.0.self_attn.q_proj.B', 'model.layers.0.self_attn.v_proj.A', 'model.layers.0.self_attn.v_proj.B', 'model.layers.1.self_attn.q_proj.A']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# LoRA implementation\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, original_linear: nn.Linear, r: int, alpha: int = None):\n",
    "        super().__init__()\n",
    "        assert isinstance(original_linear, nn.Linear)\n",
    "        self.original_linear = original_linear\n",
    "        self.original_linear.weight.requires_grad = False\n",
    "        if self.original_linear.bias is not None:\n",
    "            self.original_linear.bias.requires_grad = False\n",
    "        in_dim = original_linear.in_features\n",
    "        out_dim = original_linear.out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha if alpha else r\n",
    "\n",
    "        device = original_linear.weight.device\n",
    "        self.A = nn.Parameter(torch.empty(r, in_dim, device=device))\n",
    "        self.B = nn.Parameter(torch.zeros(out_dim, r, device=device))\n",
    "        \n",
    "        # Initialise A with He initialization\n",
    "        nn.init.kaiming_normal_(self.A, nonlinearity=\"linear\")\n",
    "\n",
    "        self.merged_weight = self.original_linear.weight\n",
    "        self.is_merged = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.is_merged:\n",
    "            return nn.functional.linear(x, self.merged_weight, self.original_linear.bias)\n",
    "        \n",
    "        base_out = self.original_linear(x)\n",
    "        lora_out = (x @ self.A.T) @ self.B.T\n",
    "        return base_out + lora_out * (self.alpha / self.r)\n",
    "    \n",
    "    def merge(self):\n",
    "        self.merged_weight = self.original_linear.weight + (self.A @ self.B.T) * (self.alpha / self.r)\n",
    "        self.is_merged = True\n",
    "\n",
    "    def unmerge(self):\n",
    "        self.is_merged = False\n",
    "\n",
    "\n",
    "model, tokenizer = load_qwen()\n",
    "\n",
    "# before applying LoRA, we need to freeze the model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "lora_rank = 4\n",
    "\n",
    "# Actually apply LoRA to the model:\n",
    "for layer in model.model.layers:\n",
    "    layer.self_attn.q_proj = LoRALinear(layer.self_attn.q_proj, r=lora_rank)\n",
    "    layer.self_attn.v_proj = LoRALinear(layer.self_attn.v_proj, r=lora_rank)\n",
    "\n",
    "\n",
    "# now lets check what weights are trainable to confirm that the LoRA has been applied\n",
    "trainable_params = [n for n, p in model.named_parameters() if p.requires_grad]\n",
    "print(f\"Here, we showcase few of the trainable parameters: {trainable_params[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55cc92c-5e9a-49dc-9984-8cde804fcdb1",
   "metadata": {},
   "source": [
    "Here we load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d64492d-b686-4fe7-9a1e-b4a523121f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data folder:\n",
    "DATA_FOLDER = os.path.join(os.path.dirname(__name__), '..', 'data')\n",
    "\n",
    "\n",
    "with h5py.File(os.path.join(DATA_FOLDER, 'lotka_volterra_data.h5'), \"r\") as f:\n",
    "    # Access the full dataset\n",
    "    trajectories = f[\"trajectories\"][:]\n",
    "    time_points = f[\"time\"][:]\n",
    "\n",
    "# Here we are only using a small fraction of the data for the experiment\n",
    "data_master = DataMaster(\n",
    "    tokenizer, trajectories, test_size=0.2, val_size=0.1, experiment_fraction=0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af342974-d635-469a-bf67-da196b532a1a",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We now proceed to train the model. For demonstration purposes, we only run a few epochs, but increasing the number of epochs can lead to better performance. This example illustrates how to train a model with LoRA using the `Trainer` class. It also serves as an opportunity to test and validate the custom `Trainer` class implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 8 inference samples with max target length 490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 4 inference samples with max target length 478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sanity check Train: 100%|██████████| 7/7 [00:00<00:00, 208.68it/s]\n",
      "Sanity check Val: 100%|██████████| 4/4 [00:00<00:00, 4004.11it/s]\n",
      "Sanity check Test: 100%|██████████| 2/2 [00:00<00:00, 1990.18it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mym429\u001b[0m (\u001b[33mym429-university-of-cambridge\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\yuche\\OneDrive\\Documents\\Brain in a vat\\CAM Mphil\\Lent\\CourseWork\\M2 Course Work\\notebooks\\wandb\\run-20250321_193006-axomsmwz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ym429-university-of-cambridge/M2-Course-Work/runs/axomsmwz' target=\"_blank\">lora-r4-lr1e-04-20250321_193006</a></strong> to <a href='https://wandb.ai/ym429-university-of-cambridge/M2-Course-Work' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ym429-university-of-cambridge/M2-Course-Work' target=\"_blank\">https://wandb.ai/ym429-university-of-cambridge/M2-Course-Work</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ym429-university-of-cambridge/M2-Course-Work/runs/axomsmwz' target=\"_blank\">https://wandb.ai/ym429-university-of-cambridge/M2-Course-Work/runs/axomsmwz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 270,336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]it/s, ce=0.5047, steps9, loss=0.5047]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1 that is less than the current step 2. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\n",
      "Validation:  25%|██▌       | 1/4 [00:05<00:17,  5.80s/it]\n",
      "\u001b[A\n",
      "Validation:  50%|█████     | 2/4 [00:11<00:11,  5.84s/it]\n",
      "\u001b[A\n",
      "Validation:  75%|███████▌  | 3/4 [00:18<00:06,  6.21s/it]\n",
      "\u001b[A\n",
      "                                                         \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation on 8 samples - MSE: 0.05341527611017227, MAE: 0.15549632906913757, Failure rate: 0.0%, Speed: 12.00 tokens/sec\n",
      "Saved checkpoint to checkpoints\\checkpoint_best.pth\n",
      "New best model with val loss: 1.4418\n",
      "Saved checkpoint to checkpoints\\checkpoint_step_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]7it/s, ce=0.5343, steps19, loss=0.5343]\n",
      "Validation:  25%|██▌       | 1/4 [00:05<00:17,  5.81s/it]\n",
      "\u001b[A\n",
      "Validation:  50%|█████     | 2/4 [00:11<00:11,  5.97s/it]\n",
      "\u001b[A\n",
      "Validation:  75%|███████▌  | 3/4 [00:18<00:06,  6.18s/it]\n",
      "\u001b[A\n",
      "                                                         \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation on 8 samples - MSE: 0.07991088926792145, MAE: 0.1819341778755188, Failure rate: 0.0%, Speed: 12.18 tokens/sec\n",
      "Saved checkpoint to checkpoints\\checkpoint_step_20.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|          | 0/4 [00:00<?, ?it/s]                                       \n",
      "Validation:  25%|██▌       | 1/4 [00:07<00:21,  7.14s/it]\n",
      "\u001b[A\n",
      "Validation:  50%|█████     | 2/4 [00:14<00:14,  7.03s/it]\n",
      "\u001b[A\n",
      "Validation:  75%|███████▌  | 3/4 [00:21<00:07,  7.34s/it]\n",
      "\u001b[A\n",
      "                                                         \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation on 8 samples - MSE: 0.07991088926792145, MAE: 0.1819341778755188, Failure rate: 0.0%, Speed: 10.38 tokens/sec\n",
      "Loading best checkpoint from checkpoints\\checkpoint_best.pth for final testing\n",
      "Warning: Loading checkpoint with weights_only=False due to: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "Loaded checkpoint from checkpoints\\checkpoint_best.pth (step 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Test:  50%|█████     | 1/2 [00:06<00:06,  6.82s/it]\n",
      "\u001b[A\n",
      "                                                   \n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test on 4 samples - MSE: 0.03215603902935982, MAE: 0.11131727695465088, Failure rate: 0.0%, Speed: 11.60 tokens/sec\n",
      "Saved checkpoint to checkpoints\\checkpoint_final.pth\n",
      "Training completed in 10 steps\n",
      "Best validation loss: 1.4418\n",
      "Final validation loss: 1.4612\n",
      "Test MSE: 0.0322, Test MAE: 0.1113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10 that is less than the current step 20. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10 that is less than the current step 20. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 10 that is less than the current step 20. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
     ]
    }
   ],
   "source": [
    "# import lib reload loraTrainer\n",
    "from importlib import reload\n",
    "import src.Trainer\n",
    "reload(src.Trainer)\n",
    "\n",
    "# tqdm clear process\n",
    "from tqdm import tqdm\n",
    "tqdm._instances.clear()\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = data_master.get_data(experiment=True, batch_size=2, target_eval_pairs = 3,\n",
    "        context_length=512)\n",
    "\n",
    "\n",
    "trainer = LoRATrainer(\n",
    "    model, train_loader=train_loader, val_loader=val_loader, test_loader=test_loader, tokenizer=tokenizer, processor = data_master.processor, \n",
    "    lora_rank =  4, context_length = 512, eval_interval=10, save_interval=10, max_steps=20,\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M2 course work  (myenv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
